{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/combined_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "train = datasets.load_dataset(\n",
    "    \"ought/raft\", \"neurips_impact_statement_risks\", split=\"train\"\n",
    ")\n",
    "validation = datasets.load_dataset(\n",
    "    \"ought/raft\", \"neurips_impact_statement_risks\", split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raft_baselines.classifiers import ChatGPTClassifier, GPT3Classifier, SetFitClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_3_classifier = GPT3Classifier(\n",
    "    train, config=\"neurips_impact_statement_risks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt_classifier = ChatGPTClassifier(\n",
    "    train, config=\"neurips_impact_statement_risks\", do_semantic_selection=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 571/571 [00:00<00:00, 137kB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)99753/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 378kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 258kB/s]\n",
      "Downloading (…)0cdb299753/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 13.9MB/s]\n",
      "Downloading (…)db299753/config.json: 100%|██████████| 571/571 [00:00<00:00, 701kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 152kB/s]\n",
      "Downloading (…)753/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 11.2MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:04<00:00, 105MB/s]  \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 15.1kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 290kB/s]\n",
      "Downloading (…)99753/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.55MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 490kB/s]\n",
      "Downloading (…)9753/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 13.4MB/s]\n",
      "Downloading (…)0cdb299753/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.78MB/s]\n",
      "Downloading (…)b299753/modules.json: 100%|██████████| 349/349 [00:00<00:00, 399kB/s]\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 5795.16 examples/s]\n",
      "Generating Training Pairs: 100%|██████████| 20/20 [00:00<00:00, 426.35it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 125\n",
      "  Total train batch size = 16\n",
      "Iteration: 100%|██████████| 125/125 [01:31<00:00,  1.37it/s]\n",
      "Epoch: 100%|██████████| 1/1 [01:31<00:00, 91.37s/it]\n"
     ]
    }
   ],
   "source": [
    "set_fit_classifier = SetFitClassifier(\n",
    "    train, config=\"neurips_impact_statement_risks\", model_head=RandomForestClassifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel, SetFitTrainer\n",
    "\n",
    "setfit_model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\", non_differentiable_model_head=RandomForestClassifier)\n",
    "\n",
    "# setfit expects data with columns ['label', 'text'], instead of ['ID', 'Impact statement', 'Label', 'Paper link', 'Paper title']\n",
    "setfit_train_data = train.map(\n",
    "    lambda example:\n",
    "        {'label': example['Label'], \n",
    "         'text': f\"Paper title: {example['Paper title']}, Impact statement: {example['Impact statement']}\"})\n",
    "\n",
    "setfit_trainer = SetFitTrainer(\n",
    "    setfit_model, setfit_train_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Training Pairs: 100%|██████████| 20/20 [00:00<00:00, 418.42it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 125\n",
      "  Total train batch size = 16\n",
      "Iteration: 100%|██████████| 125/125 [01:28<00:00,  1.41it/s]\n",
      "Epoch: 100%|██████████| 1/1 [01:28<00:00, 88.72s/it]\n"
     ]
    }
   ],
   "source": [
    "setfit_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9000, 0.1000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from raft_baselines.utils.tokenizers import GPTTokenizer\n",
    "print(setfit_model.predict_proba([\"Paper title: GNN research, Impact statement: test2\"])[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "{\"doesn't mention a harmful application\": 1.0, 'mentions a harmful application': 0.0}\n",
      "cuda\n",
      "{\"doesn't mention a harmful application\": 0.834995074353834, 'mentions a harmful application': 0.16500492564616595}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/raft-baselines/src/raft_baselines/scripts/test.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bg231ffwrqnhixu/root/raft-baselines/src/raft_baselines/scripts/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(chat_gpt_classifier\u001b[39m.\u001b[39mclassify({\u001b[39m\"\u001b[39m\u001b[39mPaper title\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mGNN research\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mImpact statement\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtest2\u001b[39m\u001b[39m\"\u001b[39m}))\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bg231ffwrqnhixu/root/raft-baselines/src/raft_baselines/scripts/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(gpt_3_classifier\u001b[39m.\u001b[39mclassify({\u001b[39m\"\u001b[39m\u001b[39mPaper title\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mGNN research\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mImpact statement\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtest2\u001b[39m\u001b[39m\"\u001b[39m}))\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bg231ffwrqnhixu/root/raft-baselines/src/raft_baselines/scripts/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(set_fit_classifier\u001b[39m.\u001b[39;49mclassify({\u001b[39m\"\u001b[39;49m\u001b[39mPaper title\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mGNN research\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mImpact statement\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtest2\u001b[39;49m\u001b[39m\"\u001b[39;49m}))\n",
      "File \u001b[0;32m~/raft-baselines/src/raft_baselines/classifiers/set_fit_classifier.py:134\u001b[0m, in \u001b[0;36mSetFitClassifier.classify\u001b[0;34m(self, target, random_seed, should_print_prompt)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m should_print_prompt:\n\u001b[1;32m    132\u001b[0m     \u001b[39mprint\u001b[39m(prompt)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_classify_prompt(prompt)\n",
      "File \u001b[0;32m~/raft-baselines/src/raft_baselines/classifiers/set_fit_classifier.py:114\u001b[0m, in \u001b[0;36mSetFitClassifier._classify_prompt\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_classify_prompt\u001b[39m(\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    111\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    112\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    113\u001b[0m     raw_p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_raw_probabilities(prompt)\n\u001b[0;32m--> 114\u001b[0m     sum_p \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msum(raw_p)\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m sum_p \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    116\u001b[0m         normalized_p \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(raw_p) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(raw_p)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/combined_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   2299\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/miniconda3/envs/combined_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:84\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "print(chat_gpt_classifier.classify({\"Paper title\": \"GNN research\", \"Impact statement\": \"test2\"}))\n",
    "print(gpt_3_classifier.classify({\"Paper title\": \"GNN research\", \"Impact statement\": \"test2\"}))\n",
    "print(set_fit_classifier.classify({\"Paper title\": \"GNN research\", \"Impact statement\": \"test2\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raft-baselines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
